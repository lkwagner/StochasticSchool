\documentclass[12pt]{article}
\usepackage{helvet}
\usepackage{fullpage}
\newcommand{\bR}{{\bf R}}
\newcommand{\br}{{\bf r}}
\newcommand{\bP}{{\bf P}}
\begin{document}

\section{The system} 

You will implement variational Monte Carlo (VMC) for the He atom. 
The algorithm works by sampling the integral:
\begin{equation}
  \label{eq:vmc}
  \langle
  E
  \rangle
  =
  \int
  \frac{\hat{H} \Psi(\bR,\bP)}{\Psi(\bR,\bP)}
  |\Psi(\bR,\bP)|^2
  =
  \frac{1}{N}
  \sum_{R_i}^N
  \lim_{N\to \infty}
  \frac{\hat{H} \Psi(\bR_i,\bP)}{\Psi(\bR_i,\bP)}
\end{equation}
where the electron positions, $\bR_i$, are sampled in the sum with probability $|\Psi(\bR_i,\bP)|^2$.
Here, $\bP$ are the variational parameters.
Once you can compute $\langle E \rangle$, you can simply explore the values of $\bP$ to find the lowest energy.
For the He atom, you'll find that this can get quite close to the exact ground state energy.

\subsection{Learning objectives}
\begin{itemize}
\item How to compute wave functions and expectation value.
\item How to sample many-body wave functions using the Metropolis algorithm.
\item The interplay between interactions and correlations. 
\end{itemize}

\section{Implementing the pieces}

Looking at the equation for $\langle E \rangle$ above, you'll see that in order to implement a variational Monte Carlo (VMC) program, we need:
\begin{itemize}
	\item $\Psi(\bR,\bP)$, $\nabla \Psi(\bR,\bP)$, and $\nabla^2 \Psi(\bR,\bP)$ (slaterwf.py)
	\item A way to compute $\frac{H\Psi(\bR,\bP)}{\Psi(\bR,\bP)}$ (hamiltonian.py)
	\item A function to generate samples with probability proportional to $|\Psi|^2$ (metropolis.py)
\end{itemize}

Each of these files has a testing program built-in that you can use to evaluate your implementation.

\subsection{Wave function object (slaterwf.py)}

A good starting point for a trial wavefunction is a Slater determinant, but because there's only one electron of each spin, it's just a product of single particle orbitals. 
For this exercise, we'll assume these orbitals are Gaussians.
\begin{equation}
\Psi(\bR)=\exp(-\alpha r_1) \exp(-\alpha r_2)	
\end{equation}

The variational parameter is $\alpha$.
In order to calculate the energy and other useful properties of the wave function, you'll need to implement \verb|value(self,pos)|, \verb|gradient(self,pos)|, and \verb|laplacian(self,pos)| that return $\Psi(\bR,\bP)$, $\nabla \Psi(\bR,\bP)$, and $\nabla^2 \Psi(\bR,\bP)$, respectively. 
Here, \verb|self| contains the parameters, $\bP$, and \verb|pos| represents an array of the positions of all the samples of both electrons. 
There is a \verb|pass| keyword wherever code is missing that you should implement. 
Do this before moving on.

\subsubsection{Testing}

Once you've finished implementing \verb|slaterwf.py|, try running the file with \verb|python slaterwf.py|. 
It should print a table with the headers \verb|delta|, \verb|derivative err|, and \verb|laplacian err|. 
This is produced by the code in the \verb|if __name__=="__main__":| section, which computes the derivatives numerically with finite difference, and compares it to your implementation. 
The \verb|delta| controls the accuracy of finite difference, and thus the \verb|err| columns should be small ($\sim 10^{-3}$) and grow very small as \verb|delta| shrinks.
If not, go back and check your code!

\subsection{Hamiltonian object (hamiltonian.py)}

Next you need to be able to compute the local energy of the wave function, defined as:
\begin{equation}
E(\bR)
\equiv
\frac{\hat{H}\Psi(\bR)}{\Psi(\bR) } = -\frac{1}{2} \sum_i \frac{\nabla_i^2 \Psi(\bR)}{\Psi(\bR)} - \sum_i \frac{2}{r_i} + \frac{1}{r_{12}},
\end{equation}
where $r_{12}=|\br_1-\br_2|$ is the distance between the two electrons.
You already have the first part from \verb|slaterwf.py|, now we just need the potential energies. 
You'll implement these in \verb|hamiltonian.py|. 
Like before, there are \verb|pass| where there is missing code. 
Implement this before continuing. 

\subsubsection{Testing}

This time, we have computed the potential energy for a few configurations by hand. 
The errors you should see should be around $10^{-9}$.
Check that your errors are this small before continuing.

\subsection{Metropolis algorithm (metropolis.py)}

The next step is to draw $\bR$ from the distribution $\Psi^2(\bR)$. 
The Metropolis-Hastings algorithm does this by the following process:
\begin{enumerate}
\item Start with an initial configuration $\bR_0$. 	
\item Propose a new configuration $\bR'=\bR_0 + \sqrt{\tau} \chi$, where $\chi$ is a gaussian random number.
\item Compute the acceptance probability $a=\frac{\Psi^2(\bR')}{\Psi^2(\bR_0)}$
\item Generate a uniform random number $u$ between 0 and 1. 
\item If $u < a$, then set $\bR_1=\bR'$. Otherwise, set $\bR_1=\bR_0$.
\end{enumerate}

The $\tau$ parameter is adjusted to maintain an efficient acceptance ratio. 
An acceptance ratio between 0.5 and 0.7 is roughly desirable for VMC; for this first implementation, if the acceptance is too high or too low, it's a signal that the samples haven't been able to move too far from their starting point.
You can play around with $\tau$ too see how it affects acceptance and the accuracy.

Once again the missing parts are currently \verb|pass|, so fill these parts in before moving on. 
Useful python tools are:
\begin{itemize}
\item np.random.randn()
\item np.random.random()
\item Conditional slices in numpy: \verb|R[:,:,u<a]=Rprime[:,:,u<a]|
\end{itemize}

\subsubsection{Testing}

The test for this function will metropolis sample a Slater wave function and check it against exact solutions. 
This test code is carrying out the integration of Eq. (\ref{eq:vmc}).

Can you figure out why these are the exact answers?
\textit{Hint}: Check out how the total energy of the wave function is being evaluated and have a look at the form of our trial wave function. 

This should also tell you why the total energy is correct even before the Metropolis algorithm is correctly implemented, or when the acceptance is zero (meaning the sample is completely random).
What happens when you change \verb|alpha| in that case?

\section{Optimizing one parameter}

At this point we have a system that can evaluate properties of a wave function.
Now you can try to figure out the following:
\begin{itemize}
\item Where is the minimum energy if we don't include electron-electron interaction? Notice anything about the errors at that point?
\item What happens to the minimum when interactions are included? Errors?
\item What is the behavior of the kinetic and potential energies as a function of $\alpha$? Do they make sense?	
\end{itemize}


\section{Add a Jastrow factor: optimizing two parameters} 

We have implemented a Jastrow wave function and an object MultiplyWF which can construct a Slater-Jastrow wave function.
You can construct it by doing
\begin{verbatim}
wf=MultiplyWF(SlaterWF(alpha),JastrowWF(beta))	
\end{verbatim}
The functional form of this simple Jastrow factor is 
\begin{equation}
\Psi_J(\bR) = \exp(\beta r_{12})	
\end{equation}

Optimize $\alpha$ with this new Jastrow. What happens to the optimal value? Why? 

\section{Computing expectation values}

\begin{itemize}
\item Radial distribution function	
\item Kinetic and potential energies. 
\end{itemize}

What happens to the average distance between electrons with the Jastrow factor? 

How does the Jastrow factor affect 

\section{The linear method for optimization} 

\begin{itemize}
\item Expectation values we need
\item Testing 
\end{itemize}

\subsection{Programming parameter derivatives}


\section{Improving the sampling: biased moves} 

\section{Excited states: triplet. } 

\end{document}
